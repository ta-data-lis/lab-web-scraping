{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools or clicking in 'Inspect' on any browser. Here is an example:\n",
    "\n",
    "![title](example_1.png)\n",
    "\n",
    "2. Use BeautifulSoup `find_all()` to extract all the html elements that contain the developer names. Hint: pass in the `attrs` parameter to specify the class.\n",
    "\n",
    "3. Loop through the elements found and get the text for each of them.\n",
    "\n",
    "4. While you are at it, use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names. Hint: you may also use `.get_text()` instead of `.text` and pass in the desired parameters to do some string manipulation (check the documentation).\n",
    "\n",
    "5. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anthony Sottile (asottile)',\n",
       " 'Michael Lynch (mtlynch)',\n",
       " 'Michał Krassowski (krassowski)',\n",
       " 'Mladen Macanović (stsrki)',\n",
       " 'Arvid Norberg (arvidn)',\n",
       " 'Dries Vints (driesvints)',\n",
       " 'Blake Blackshear (blakeblackshear)',\n",
       " 'Miek Gieben (miekg)',\n",
       " 'Pierre Krieger (tomaka)',\n",
       " 'isaacs (isaacs)',\n",
       " 'Julien Danjou (jd)',\n",
       " 'Wei Wang (onevcat)',\n",
       " 'Henrik Rydgård (hrydgard)',\n",
       " 'Michael Waskom (mwaskom)',\n",
       " 'Stephen Celis (stephencelis)',\n",
       " 'pedrobern (PedroBern)',\n",
       " 'Guillaume Gomez (GuillaumeGomez)',\n",
       " 'Michael Teeuw (MichMich)',\n",
       " 'Kévin Dunglas (dunglas)',\n",
       " 'Filippo Valsorda (FiloSottile)',\n",
       " 'Diamond Lewis (dplewis)',\n",
       " 'Felix Angelov (felangel)',\n",
       " 'van Hauser (vanhauser-thc)',\n",
       " 'snipe (snipe)',\n",
       " 'Himanshu Mishra (OrkoHunter)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "people_tags = soup.find_all('h1', {\"class\": \"h3 lh-condensed\"})\n",
    "\n",
    "people = [tag.a.text.strip() for tag in people_tags]\n",
    "\n",
    "people_nick_tags = soup.find_all('p', {\"class\": \"f4\"})\n",
    "people_nick = [tag.text.strip() for tag in people_nick_tags][1:]\n",
    "\n",
    "hot_devs = [people[i] + ' (' + people_nick[i] + ')' for i in range(len(people))]\n",
    "hot_devs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('explosion', 'spaCy'),\n",
       " ('doino-gretchenliev', 'revolut-stocks'),\n",
       " ('TheAlgorithms', 'Python'),\n",
       " ('MathInspector', 'MathInspector'),\n",
       " ('swisskyrepo', 'PayloadsAllTheThings'),\n",
       " ('dupontgu', 'retro-ipod-spotify-client'),\n",
       " ('alibaba', 'taobao-iphone-device'),\n",
       " ('3b1b', 'manim'),\n",
       " ('minimaxir', 'big-list-of-naughty-strings'),\n",
       " ('sdushantha', 'wifi-password'),\n",
       " ('tensortrade-org', 'tensortrade'),\n",
       " ('davidbombal', 'red-python-scripts'),\n",
       " ('HashPals', 'Name-That-Hash'),\n",
       " ('tiangolo', 'fastapi'),\n",
       " ('PeterL1n', 'BackgroundMattingV2'),\n",
       " ('yanshengjia', 'ml-road'),\n",
       " ('optas', 'artemis'),\n",
       " ('huggingface', 'transformers'),\n",
       " ('ultrafunkamsterdam', 'undetected-chromedriver'),\n",
       " ('y1ndan', 'genshin-impact-helper'),\n",
       " ('ZHKKKe', 'MODNet'),\n",
       " ('numpy', 'numpy'),\n",
       " ('owid', 'covid-19-data'),\n",
       " ('aws', 'aws-cli'),\n",
       " ('vt-vl-lab', 'FGVC')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "repos_tags = soup.find_all('h1', {\"class\": \"h3 lh-condensed\"})\n",
    "\n",
    "repos_users = [tag.span.text.strip()[:-2] for tag in repos_tags]\n",
    "repos_names = [tag.a.text.replace('\\n', '').split('/')[-1].strip() for tag in repos_tags]\n",
    "\n",
    "repos = list(zip(repos_users, repos_names))\n",
    "repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Display all the image links from Walt Disney wikipedia page.\n",
    "Hint: use `.get()` to access information inside tags. Check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/footer/wikimedia-button.png',\n",
       " '/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "find_main_page = soup.find_all('img')\n",
    "images = [tag.get('src') for tag in find_main_page]\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('English', '6.240.000+ articles'),\n",
       " ('日本語', '1.251.000+ 記事'),\n",
       " ('Deutsch', '2.531.000+ Artikel'),\n",
       " ('Español', '1.657.000+ artículos'),\n",
       " ('Русский', '1.695.000+ статей'),\n",
       " ('Français', '2.294.000+ articles'),\n",
       " ('Italiano', '1.671.000+ voci'),\n",
       " ('中文', '1.174.000+ 條目'),\n",
       " ('Polski', '1.453.000+ haseł'),\n",
       " ('Português', '1.054.000+ artigos')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "languages_tags = soup.find_all('div', {'class': 'central-featured-lang'})\n",
    "\n",
    "languages = [tag.strong.text for tag in languages_tags]\n",
    "articles = [tag.small.text.replace('\\xa0', '.') for tag in languages_tags]\n",
    "\n",
    "lang_art = list(zip(languages, articles))\n",
    "lang_art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Display the top 10 languages by number of native speakers stored in a pandas dataframe.\n",
    "Hint: After finding the correct table you want to analyse, you can use a nested **for** loop to find the elements row by row (check out the 'td' and 'tr' tags). <br>An easier way to do it is using pd.read_html(), check out documentation [here](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_html.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>Percentageof world pop.(March 2019)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "      <td>11.922%</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "      <td>5.994%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "      <td>4.922%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (sanskritised Hindustani)</td>\n",
       "      <td>341</td>\n",
       "      <td>4.429%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>228</td>\n",
       "      <td>2.961%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>Czech</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0.139%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>Taʽizzi-Adeni Arabic</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.136%</td>\n",
       "      <td>Afroasiatic</td>\n",
       "      <td>Semitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>Uyghur</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.135%</td>\n",
       "      <td>Turkic</td>\n",
       "      <td>Karluk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>Min Dong Chinese</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.134%</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>Sylheti</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.134%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                         Language Speakers(millions)  \\\n",
       "0     1                 Mandarin Chinese                918   \n",
       "1     2                          Spanish                480   \n",
       "2     3                          English                379   \n",
       "3     4  Hindi (sanskritised Hindustani)                341   \n",
       "4     5                          Bengali                228   \n",
       "..  ...                              ...                ...   \n",
       "86   87                            Czech               10.7   \n",
       "87   88             Taʽizzi-Adeni Arabic               10.5   \n",
       "88   89                           Uyghur               10.4   \n",
       "89   90                 Min Dong Chinese               10.3   \n",
       "90   91                          Sylheti               10.3   \n",
       "\n",
       "   Percentageof world pop.(March 2019) Language family        Branch  \n",
       "0                              11.922%    Sino-Tibetan       Sinitic  \n",
       "1                               5.994%   Indo-European       Romance  \n",
       "2                               4.922%   Indo-European      Germanic  \n",
       "3                               4.429%   Indo-European    Indo-Aryan  \n",
       "4                               2.961%   Indo-European    Indo-Aryan  \n",
       "..                                 ...             ...           ...  \n",
       "86                              0.139%   Indo-European  Balto-Slavic  \n",
       "87                              0.136%     Afroasiatic       Semitic  \n",
       "88                              0.135%          Turkic        Karluk  \n",
       "89                              0.134%    Sino-Tibetan       Sinitic  \n",
       "90                              0.134%   Indo-European    Indo-Aryan  \n",
       "\n",
       "[91 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "table = soup.find_all('table', attrs = {'class':'wikitable'})[0]\n",
    "rows = table.find_all('td')\n",
    "columns = table.find_all('th')\n",
    "\n",
    "language_table = [tag.text.replace('\\n', '')[:-3] if '[' in tag.text.replace('\\n', '') else tag.text.replace('\\n', '') for tag in rows]\n",
    "\n",
    "columns = [tag.text.replace('\\n', '')[:-3] if '[' in tag.text.replace('\\n', '') else tag.text.replace('\\n', '') for tag in columns]\n",
    "\n",
    "languages_dict = {columns[i]:language_table[i::len(columns)] for i in range(len(columns))}\n",
    "\n",
    "languages_df = pd.DataFrame(languages_dict)\n",
    "languages_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe.\n",
    "Hint: If you hover over the title of the movie, you should see the director's name. Can you find where it's stored in the html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movies</th>\n",
       "      <th>release</th>\n",
       "      <th>directors</th>\n",
       "      <th>star1</th>\n",
       "      <th>star2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Os Condenados de Shawshank</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>Tim Robbins</td>\n",
       "      <td>Morgan Freeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O Padrinho</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Marlon Brando</td>\n",
       "      <td>Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O Padrinho: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O Cavaleiro das Trevas</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doze Homens em Fúria</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Henry Fonda</td>\n",
       "      <td>Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>A Batalha de Argel</td>\n",
       "      <td>1966</td>\n",
       "      <td>Gillo Pontecorvo</td>\n",
       "      <td>Brahim Hadjadj</td>\n",
       "      <td>Jean Martin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Tangerinas</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zaza Urushadze</td>\n",
       "      <td>Lembit Ulfsak</td>\n",
       "      <td>Elmo Nüganen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Koe no katachi</td>\n",
       "      <td>2016</td>\n",
       "      <td>Naoko Yamada</td>\n",
       "      <td>Miyu Irino</td>\n",
       "      <td>Saori Hayami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Shin seiki Evangelion Gekijô-ban: Air/Magokoro...</td>\n",
       "      <td>1997</td>\n",
       "      <td>Hideaki Anno</td>\n",
       "      <td>Megumi Ogata</td>\n",
       "      <td>Megumi Hayashibara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Milagre na Cela 7</td>\n",
       "      <td>2019</td>\n",
       "      <td>Mehmet Ada Öztekin</td>\n",
       "      <td>Aras Bulut Iynemli</td>\n",
       "      <td>Nisa Sofiya Aksongur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                movies  release  \\\n",
       "0                           Os Condenados de Shawshank     1994   \n",
       "1                                           O Padrinho     1972   \n",
       "2                                 O Padrinho: Parte II     1974   \n",
       "3                               O Cavaleiro das Trevas     2008   \n",
       "4                                 Doze Homens em Fúria     1957   \n",
       "..                                                 ...      ...   \n",
       "245                                 A Batalha de Argel     1966   \n",
       "246                                         Tangerinas     2013   \n",
       "247                                     Koe no katachi     2016   \n",
       "248  Shin seiki Evangelion Gekijô-ban: Air/Magokoro...     1997   \n",
       "249                                  Milagre na Cela 7     2019   \n",
       "\n",
       "                directors               star1                 star2  \n",
       "0          Frank Darabont         Tim Robbins        Morgan Freeman  \n",
       "1    Francis Ford Coppola       Marlon Brando             Al Pacino  \n",
       "2    Francis Ford Coppola           Al Pacino        Robert De Niro  \n",
       "3       Christopher Nolan      Christian Bale          Heath Ledger  \n",
       "4            Sidney Lumet         Henry Fonda           Lee J. Cobb  \n",
       "..                    ...                 ...                   ...  \n",
       "245      Gillo Pontecorvo      Brahim Hadjadj           Jean Martin  \n",
       "246        Zaza Urushadze       Lembit Ulfsak          Elmo Nüganen  \n",
       "247          Naoko Yamada          Miyu Irino          Saori Hayami  \n",
       "248          Hideaki Anno        Megumi Ogata    Megumi Hayashibara  \n",
       "249    Mehmet Ada Öztekin  Aras Bulut Iynemli  Nisa Sofiya Aksongur  \n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "table_data = soup.find_all('td')\n",
    "\n",
    "movies = [t.find('img')['alt'] for t in table_data if t.find('img')]\n",
    "\n",
    "peoples = [t.a.get('title').replace(' (dir.)', '').split(', ') for t in table_data if (t.a and t.a.get('title'))]\n",
    "\n",
    "directors = [x[0] for x in peoples]\n",
    "star1 = [x[1] for x in peoples]\n",
    "star2 = [x[2] for x in peoples]\n",
    "year = [int(t.text[1:-1]) for t in soup.find_all('span', {'class':'secondaryInfo'})]\n",
    "\n",
    "\n",
    "imdb = pd.DataFrame(list(zip(movies, year, directors, star1, star2)), columns= ['movies', 'release', 'directors', 'star1', 'star2'])\n",
    "\n",
    "imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'https://www.imdb.com/list/ls009796553/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le Mans '66: O Duelo</td>\n",
       "      <td>2019</td>\n",
       "      <td>American car designer Carroll Shelby and drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Ponte do Rio Kwai</td>\n",
       "      <td>1957</td>\n",
       "      <td>British POWs are forced to build a railway bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Grande Evasão</td>\n",
       "      <td>1963</td>\n",
       "      <td>Allied prisoners of war plan for several hundr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O Ódio</td>\n",
       "      <td>1995</td>\n",
       "      <td>24 hours in the lives of three young men in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Horizontes de Glória</td>\n",
       "      <td>1957</td>\n",
       "      <td>After refusing to attack an enemy position, a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Matou</td>\n",
       "      <td>1931</td>\n",
       "      <td>When the police in a German city are unable to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O Sexto Sentido</td>\n",
       "      <td>1999</td>\n",
       "      <td>A boy who communicates with spirits seeks the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Koe no katachi</td>\n",
       "      <td>2016</td>\n",
       "      <td>A young man is ostracized by his classmates af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sacanas Sem Lei</td>\n",
       "      <td>2009</td>\n",
       "      <td>In Nazi-occupied France during World War II, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vem e Vê</td>\n",
       "      <td>1985</td>\n",
       "      <td>After finding an old rifle, a young boy joins ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title  year  \\\n",
       "0  Le Mans '66: O Duelo  2019   \n",
       "1   A Ponte do Rio Kwai  1957   \n",
       "2       A Grande Evasão  1963   \n",
       "3                O Ódio  1995   \n",
       "4  Horizontes de Glória  1957   \n",
       "5                 Matou  1931   \n",
       "6       O Sexto Sentido  1999   \n",
       "7        Koe no katachi  2016   \n",
       "8       Sacanas Sem Lei  2009   \n",
       "9              Vem e Vê  1985   \n",
       "\n",
       "                                             summary  \n",
       "0  American car designer Carroll Shelby and drive...  \n",
       "1  British POWs are forced to build a railway bri...  \n",
       "2  Allied prisoners of war plan for several hundr...  \n",
       "3  24 hours in the lives of three young men in th...  \n",
       "4  After refusing to attack an enemy position, a ...  \n",
       "5  When the police in a German city are unable to...  \n",
       "6  A boy who communicates with spirits seeks the ...  \n",
       "7  A young man is ostracized by his classmates af...  \n",
       "8  In Nazi-occupied France during World War II, a...  \n",
       "9  After finding an old rifle, a young boy joins ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "import random\n",
    "\n",
    "links = list(set(['https://www.imdb.com' + t.a['href'] for t in table_data if t.a]))\n",
    "\n",
    "samples = random.sample(links, 10)\n",
    "\n",
    "movie_title = []\n",
    "movie_year = []\n",
    "movie_summary = []\n",
    "\n",
    "\n",
    "for title in samples:\n",
    "    \n",
    "    url = title\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    movie_data = soup.find_all('h1')\n",
    "    mdata = [t.text.strip().split('\\xa0') for t in movie_data][0]\n",
    "    movie_title += [mdata[0]]\n",
    "    movie_year += [int(mdata[1].replace('(', '').replace(')', ''))]\n",
    "\n",
    "    summary = soup.find_all('div', {'class': 'summary_text'})\n",
    "    movie_summary += [[t.text.strip() for t in summary][0]]\n",
    "    \n",
    "random_movies = {'title': movie_title, 'year': movie_year, 'summary': movie_summary}\n",
    "random_movies = pd.DataFrame(random_movies)\n",
    "random_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 100 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe.\n",
    "***Hint:*** Here the displayed number of earthquakes per page is 20, but you can easily move to the next page by looping through the desired number of pages and adding it to the end of the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/?view='\n",
    "\n",
    "# This is how you will loop through each page:\n",
    "number_of_pages = int(100/20)\n",
    "each_page_urls = []\n",
    "\n",
    "for n in range(1, number_of_pages+1):\n",
    "    link = url+str(n)\n",
    "    each_page_urls.append(link)\n",
    "    \n",
    "each_page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
